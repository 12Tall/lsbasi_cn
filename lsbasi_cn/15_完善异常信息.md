15_完善错误信息    

📅 2019-06-21  

> “我走得很慢，但我从不后退” —— 亚伯拉罕·林肯    

现在我们回归正常的编程日程！:)  

在开始识别和解释过程调用的话题之前，我们先来改进一下我们现有代码的错误提示部分。迄今为止，当我们的编译器在词法分析、语法分析或者语义分析中遇到错误时，就会直接抛出一个通用的异常并将堆栈追踪的信息一并给出。然而我们可以做得更好！  

为了能更加精确地定位代码中出现问题的位置，我们需要给解释器/编译器加一些新的特性，顺便也可以做一些其他的改进。这将让我们的解释器对用户更加友好，在“短暂”（一年）的停更后展示一下我们的技术，并且也有利于我们以后向编译器/解释器中添加更多的功能。  

## 今天的目标  
- 改进词法分析器、语法分析器、语义分析器的错误报告功能。用类似于`SyntaxError: Unexpected token -> Token(TokenType.SEMI, ‘;’, position=23:13)` 的错误信息替代错误栈中的`Invalid syntax`；  
- 新增`--scope` 命令行选项，来控制是否打印作用域的调试信息；  
- 从今天开始，我们的代码将全部迁移至Python 3，并且所有的源码只在Python 3.7+ 上测试。  

## 修改词法分析器  
首先从词法分析器入手，来展示一下我们的代码实力！下面是需要改动的点：  
1. 新增错误码（Error Code），和自定义异常：`LexerError`、`ParserError` 和`SemanticError`；  
2. 给词法分析器新增记录`Token` 位置的属性：`lineno` 和`column`；  
3. 更新`advance` 方法，用于更新词法分析器对象中的`lineno` 和`column` 属性；  
4. 更新词法分析器中的`error` 方法以抛出`LexerError` 的异常信息（包含当前所在的行和列的信息）；  
5. 通过枚举类型定义`TokenType`（Python 3.4+）；  
6. 通过`TokenType` 枚举类型自动创建保留关键字；  
7. 给Token 类新增属性：`lineno` 和`column`，用于记录`token` 对象所在的行列信息；  
8. 我们会重构`get_next_token` 方法，让代码更简短，并且写一个更一般的方法来处理所有单字符的token。  

我们首先来定义一些错误代码，这些错误代码将会在词法分析、语法分析以及语义分析中用到。同时我们也定义对应的错误类：`LexerError`、`ParserError` 和`SemanticError`：  
```python
from enum import Enum

class ErrorCode(Enum):
    UNEXPECTED_TOKEN = 'Unexpected token'
    ID_NOT_FOUND     = 'Identifier not found'
    DUPLICATE_ID     = 'Duplicate id found'

class Error(Exception):
    def __init__(self, error_code=None, token=None, message=None):
        self.error_code = error_code
        self.token = token
        # add exception class name before the message
        self.message = f'{self.__class__.__name__}: {message}'

class LexerError(Error):
    pass

class ParserError(Error):
    pass

class SemanticError(Error):
    pass
```

`ErrorCode` 是一个枚举类型，每个枚举属性都具有一个值：  
```shell-session
>>> from enum import Enum
>>>
>>> class ErrorCode(Enum):
...     UNEXPECTED_TOKEN = 'Unexpected token'
...     ID_NOT_FOUND     = 'Identifier not found'
...     DUPLICATE_ID     = 'Duplicate id found'
...
>>> ErrorCode
<enum 'ErrorCode'>
>>>
>>> ErrorCode.ID_NOT_FOUND
<ErrorCode.ID_NOT_FOUND: 'Identifier not found'>
```  

`Error` 基类的构造函数接受三个参数：  
- `error_code`：枚举类型  
- `token`：Token 的实例对象  
- `message`：更详细的错误信息  

正如之前所提到的，`LexerError` 用来专指词法分析器的错误，`ParserError` 用来代表语法分析器的错误，而`SemanticError` 用来表示语义分析中遇到的错误。  

然后为了更好地表示错误信息，我们需要获取错误在源代码中的位置。为此，我们需要在词法分析器生成`token` 时添加当前行和列的信息，这些信息会一直随着`token`：  
```python
class Lexer(object):
    def __init__(self, text):
        ...
        # self.pos is an index into self.text
        self.pos = 0
        self.current_char = self.text[self.pos]
        # token line number and column number
        self.lineno = 1
        self.column = 1
```

下一步我们需要在遇到换行符时重新设置`lineno` 和`column` 计数，`self.pos` 依然需要自增1：  
```python
def advance(self):
    """Advance the `pos` pointer and set the `current_char` variable."""
    if self.current_char == '\n':
        self.lineno += 1
        self.column = 0

    self.pos += 1
    if self.pos > len(self.text) - 1:
        self.current_char = None  # Indicates end of input
    else:
        self.current_char = self.text[self.pos]
        self.column += 1
```  
通过以上改动，词法分析器产生的每一个`Token` 都会带有其在源马中的行和列的信息。  

最后是抛出异常的`error` 方法，因为是在生成`token` 时遇到异常，所以不会包含`token` 信息：  
```python
def error(self):
    s = "Lexer error on '{lexeme}' line: {lineno} column: {column}".format(
        lexeme=self.current_char,
        lineno=self.lineno,
        column=self.column,
    )
    raise LexerError(message=s)
```

以上是`LexerError` 异常处理的部分。下面我们想利用枚举类重构一下`Token` 类型相关的代码，因为目前它们都是以模块变量的形式写在源文件中的。我们需要创建一个`TokenType` 类，后面可以看到它将有利于我们简化代码。  

这是目前的代码风格：  
```python
# Token types
PLUS  = 'PLUS'
MINUS = 'MINUS'
MUL   = 'MUL'
...
```  

这是新的代码风格：
```python
# 译注：
# Python 中的枚举属性是按定义时的顺序排列的，所以可以放心地进行遍历操作  
class TokenType(Enum):
    # single-character token types
    PLUS          = '+'
    MINUS         = '-'
    MUL           = '*'
    FLOAT_DIV     = '/'
    LPAREN        = '('
    RPAREN        = ')'
    SEMI          = ';'
    DOT           = '.'
    COLON         = ':'
    COMMA         = ','
    # block of reserved words
    PROGRAM       = 'PROGRAM'  # marks the beginning of the block
    INTEGER       = 'INTEGER'
    REAL          = 'REAL'
    INTEGER_DIV   = 'DIV'
    VAR           = 'VAR'
    PROCEDURE     = 'PROCEDURE'
    BEGIN         = 'BEGIN'
    END           = 'END'      # marks the end of the block
    # misc
    ID            = 'ID'
    INTEGER_CONST = 'INTEGER_CONST'
    REAL_CONST    = 'REAL_CONST'
    ASSIGN        = ':='
    EOF           = 'EOF'
```

过去在添加新的`token` 类型时，我们需要手动创建`RESERVED_KEYWORDS` 关键字字典，例如：  
- a. 首先在模块层面声明变量：`STRING = 'STRING'`  
- b. 然后将其手工添加进`RESERVED_KEYWORDS` 字典中  

现在我们有了TokenType 枚举类型，就可以让步骤b 自动执行了。这就是[two is too many](https://www.codesimplicity.com/post/two-is-too-many/) 规则。今后添加`token` 类型时，我们只需在`TokenType` 类的`PROGRAM` 和`END` 关键字之间添加对应的关键字就好了，剩下的事情`_build_reserved_keywords` 方法会帮我们处理：   
```python  
def _build_reserved_keywords():
    """Build a dictionary of reserved keywords.

    The function relies on the fact that in the TokenType
    enumeration the beginning of the block of reserved keywords is
    marked with PROGRAM and the end of the block is marked with
    the END keyword.

    Result:
        {'PROGRAM': <TokenType.PROGRAM: 'PROGRAM'>,
         'INTEGER': <TokenType.INTEGER: 'INTEGER'>,
         'REAL': <TokenType.REAL: 'REAL'>,
         'DIV': <TokenType.INTEGER_DIV: 'DIV'>,
         'VAR': <TokenType.VAR: 'VAR'>,
         'PROCEDURE': <TokenType.PROCEDURE: 'PROCEDURE'>,
         'BEGIN': <TokenType.BEGIN: 'BEGIN'>,
         'END': <TokenType.END: 'END'>}
    """
    # enumerations support iteration, in definition order
    tt_list = list(TokenType)  # 想想枚举属性的定义顺序
    start_index = tt_list.index(TokenType.PROGRAM)
    end_index = tt_list.index(TokenType.END)
    reserved_keywords = {  # 字典推导式，语法糖
        token_type.value: token_type
        for token_type in tt_list[start_index:end_index + 1]
    }
    return reserved_keywords


RESERVED_KEYWORDS = _build_reserved_keywords()
```
从上面函数的说明文档中可以看出，生成字典的内容依赖于`TokenType` 中定义在`PROGRAM` 和`END` 关键字之间的内容。该方法首先将`TokenType` 转化为一个`list`（注意元素与定义顺序相同），然后获取`PROGRAM` 和`END` 关键字的索引，最后根据索引，利用字典推导式的特性来创建保留关键字的字典：  
```shell-session
>>> from spi import _build_reserved_keywords
>>> from pprint import pprint
>>> pprint(_build_reserved_keywords())  # 'pprint' sorts the keys
{'BEGIN': <TokenType.BEGIN: 'BEGIN'>,
 'DIV': <TokenType.INTEGER_DIV: 'DIV'>,
 'END': <TokenType.END: 'END'>,
 'INTEGER': <TokenType.INTEGER: 'INTEGER'>,
 'PROCEDURE': <TokenType.PROCEDURE: 'PROCEDURE'>,
 'PROGRAM': <TokenType.PROGRAM: 'PROGRAM'>,
 'REAL': <TokenType.REAL: 'REAL'>,
 'VAR': <TokenType.VAR: 'VAR'>}
```  

接下来是更新`Token` 类，新增`lineno` 和`column` 属性：  
```python
class Token(object):
    def __init__(self, type, value, lineno=None, column=None):
        self.type = type
        self.value = value
        self.lineno = lineno
        self.column = column

    def __str__(self):
        """String representation of the class instance.

        Example:
            >>> Token(TokenType.INTEGER, 7, lineno=5, column=10)
            Token(TokenType.INTEGER, 7, position=5:10)
        """
        return 'Token({type}, {value}, position={lineno}:{column})'.format(
            type=self.type,
            value=repr(self.value),
            lineno=self.lineno,
            column=self.column,
        )

    def __repr__(self):
        return self.__str__()
```

最后进入`get_next_token` 方法。因为Python 中枚举的特性，我们可以在创建单字符`token` 时减少大量的冗余代码。  
这是我们之前的风格：  
```python
if self.current_char == ';':
    self.advance()
    return Token(SEMI, ';')

if self.current_char == ':':
    self.advance()
    return Token(COLON, ':')

if self.current_char == ',':
    self.advance()
    return Token(COMMA, ',')
...
```  

现在我们可以采用更一般的代码来处理所有单字符的`token` 了：  
```python  
# single-character token
try:
    # get enum member by value, e.g.
    # TokenType(';') --> TokenType.SEMI
    token_type = TokenType(self.current_char)
except ValueError:
    # no enum member with value equal to self.current_char
    self.error()
else:
    # create a token with a single-character lexeme as its value
    token = Token(
        type=token_type,
        value=token_type.value,  # e.g. ';', '.', etc
        lineno=self.lineno,
        column=self.column,
    )
    self.advance()
    return token  
"""
译注：  
1) 此代码是放在所有多字符token 处理的最后面的，也就是说如果这段代码走完还没有识别出token，就肯定是出错了。  
2) 虽然此代码功能上没问题，但是使用try-catch 做判断这一行为是极不推荐的，因为异常处理会极大的牺牲代码性能！这里可以仿效生成保留字字典一样，生成单字符token 的字典进行处理。  
3) 如果想要兼容多字符的信息，可能状态机也是个不错的选择。    
"""
```  

按理说，`if` 代码块的可读性要更高一些，但是当你理解了上述代码之后，你会发现原理非常简单。Python 允许我们通过枚举的值找到枚举的成员，然后我们就可以：  
- 试图通过当前字符查找其在枚举中是否有定义  
- 如果没有就抛出异常  
- 如果有就返回一个单字符`token`  

这个代码块将会处理所有单字符的`token`，我们所需要做的仅仅是向枚举类型中添加新的定义即可。  

看到没，这其实是一个双赢的局面：我们学习了Python 的枚举类型，尤其是如何通过值访问枚举成员；还写出了处理单字符`token` 的通用方法，并且减少了很多冗余代码。  


## 语法分析器  
上面是词法分析器的修改，下面我们来看语法分析器的：  
1. 更新语法分析器`error` 方法，使其可以抛出`ParserError` 异常（包含`token` 信息）  
2. 更新`eat` 方法，使之能调用新的`error` 方法  
3. 重构`declarations` 方法，将过程声明部分单独提取为`procedure_declaration`   

首先，我们更新error 方法：  
```python
def error(self, error_code, token):
    raise ParserError(
        error_code=error_code,
        token=token,
        message=f'{error_code.value} -> {token}',
    )
```  

然后是`eat` 方法，`token` 类型不匹配则报错：  
```python
def eat(self, token_type):
    # compare the current token type with the passed token
    # type and if they match then "eat" the current token
    # and assign the next token to the self.current_token,
    # otherwise raise an exception.
    if self.current_token.type == token_type:
        self.current_token = self.get_next_token()
    else:
        self.error(
            error_code=ErrorCode.UNEXPECTED_TOKEN,
            token=self.current_token,
        )
```  

最后是提取`procedure_declaration` 部分：  
```python  
def declarations(self):
    """
    declarations : (VAR (variable_declaration SEMI)+)? procedure_declaration*
    """
    declarations = []

    if self.current_token.type == TokenType.VAR:
        self.eat(TokenType.VAR)
        while self.current_token.type == TokenType.ID:
            var_decl = self.variable_declaration()
            declarations.extend(var_decl)
            self.eat(TokenType.SEMI)

    while self.current_token.type == TokenType.PROCEDURE:
        proc_decl = self.procedure_declaration()
        declarations.append(proc_decl)

    return declarations

def procedure_declaration(self):
    """procedure_declaration :
         PROCEDURE ID (LPAREN formal_parameter_list RPAREN)? SEMI block SEMI
    """
    self.eat(TokenType.PROCEDURE)
    proc_name = self.current_token.value
    self.eat(TokenType.ID)
    params = []

    if self.current_token.type == TokenType.LPAREN:
        self.eat(TokenType.LPAREN)
        params = self.formal_parameter_list()
        self.eat(TokenType.RPAREN)

    self.eat(TokenType.SEMI)
    block_node = self.block()
    proc_decl = ProcedureDecl(proc_name, params, block_node)
    self.eat(TokenType.SEMI)
    return proc_decl
```  

以上是所有语法分析器的改动，下面看语义分析器的。  

## 语义分析器  
最终，语义分析器的改动如下：  
1. 新增`error` 方法，用于抛出语义异常  
2. 更新`visit_VarDecl` 方法，以检查是否存在重复声明  
3. 更新`visit_Var` 方法，以检查是否存在未经声明的变量调用  
4. 给`ScopedSymbolTable` 和`SemanticAnalyzer` 增加`log` 方法打印调试信息  
5. 通过给命令行增减`--scope` 选项，以控制`log` 函数的开关  
6. 添加空方法`visit_Num` 和`visit_UnaryOp`  

首先，我们添加`error` 方法，以抛出语义错误：  
```python
def error(self, error_code, token):
    raise SemanticError(
        error_code=error_code,
        token=token,
        message=f'{error_code.value} -> {token}',
    )
```

然后修改`visit_VarDecl` 以检查重复声明的语义错误：  
```python  
def visit_VarDecl(self, node):
    type_name = node.type_node.value
    type_symbol = self.current_scope.lookup(type_name)

    # We have all the information we need to create a variable symbol.
    # Create the symbol and insert it into the symbol table.
    var_name = node.var_node.value
    var_symbol = VarSymbol(var_name, type_symbol)

    # Signal an error if the table already has a symbol
    # with the same name
    if self.current_scope.lookup(var_name, current_scope_only=True):
        self.error(
            error_code=ErrorCode.DUPLICATE_ID,
            token=node.var_node.token,
        )

    self.current_scope.insert(var_symbol)
```  

同样地，更新`visit_Var` 方法：  
```python  
def visit_Var(self, node):
    var_name = node.value
    var_symbol = self.current_scope.lookup(var_name)
    if var_symbol is None:
        self.error(error_code=ErrorCode.ID_NOT_FOUND, token=node.token)
```  

语义异常将会以下面的格式打印  
```shel-session  
SemanticError: Duplicate id found -> Token(TokenType.ID, 'a', position=21:4)  
## 或者 ##
SemanticError: Identifier not found -> Token(TokenType.ID, 'b', position=22:9)
```  

接着，我们给`ScopedSymbolTable` 和`SemanticAnalyzer` 增加`log` 方法来替换`print` 语句：  
```python  
def log(self, msg):
    if _SHOULD_LOG_SCOPE:
        print(msg)
```  
可以看到，只有当全局变量`_SHOULD_LOG_SCOPE` 为真时，错误信息才会被打印。而我们可以在命令行中，通过指定参数`--scope` 来设置此变量为真：  
```python  
parser = argparse.ArgumentParser(  # python 的标准库中
    description='SPI - Simple Pascal Interpreter'
)
parser.add_argument('inputfile', help='Pascal source file')
parser.add_argument(
    '--scope',
    help='Print scope information',
    action='store_true',  # 一旦检测到该参数就设置为真
)
args = parser.parse_args()
global _SHOULD_LOG_SCOPE
_SHOULD_LOG_SCOPE = args.scope
```  
实际运行情况如下：  
```shell-session  
$ python spi.py idnotfound.pas --scope
ENTER scope: global
Insert: INTEGER
Insert: REAL
Lookup: INTEGER. (Scope name: global)
Lookup: a. (Scope name: global)
Insert: a
Lookup: b. (Scope name: global)
SemanticError: Identifier not found -> Token(TokenType.ID, 'b', position=6:9)
```  
默认情况下log 功能是关闭的：  
```shell-session  
$ python spi.py idnotfound.pas
SemanticError: Identifier not found -> Token(TokenType.ID, 'b', position=6:9)
```

最后添加空方法`visit_Num` 和`visit_UnaryOp`    
```python  
def visit_Num(self, node):
    pass

def visit_UnaryOp(self, node):
    pass
```  

以上是语义分析器的所有改动部分。  

从[GitHub](https://github.com/rspivak/lsbasi/tree/master/part15/) 可以下载包含各种类型的错误的Pascal 源码，可以试着用新的解释器验证一下，看会产生什么样的错误信息。  

以上就是今天的全部内容，你可以在上面GitHub 的仓库中找到本文所有的代码。下一章，我们将会介绍如何识别/解析过程调用。敬请期待，再见！

-----  
2022-07-12 18:10